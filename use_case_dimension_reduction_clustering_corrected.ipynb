{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "464748fa",
   "metadata": {},
   "source": [
    "\n",
    "# Clustering text documents using k-means\n",
    "\n",
    "This is an example showing how the scikit-learn API can be used to cluster\n",
    "documents by topics using a [Bag of Words\n",
    "approach](https://en.wikipedia.org/wiki/Bag-of-words_model).\n",
    "\n",
    "Two algorithms are demonstrated, namely `KMeans` and its more scalable variant,\n",
    "`MiniBatchKMeans`. Additionally, latent semantic analysis is used to reduce\n",
    "dimensionality and discover latent patterns in the data.\n",
    "\n",
    "## Loading text data\n",
    "\n",
    "We load data from 20 newgroups dataset, which comprises around 18,000 newsgroups posts\n",
    "on 20 topics. For illustrative purposes and to reduce the computational cost, we\n",
    "select a subset of 4 topics only accounting for around 3,400 documents.\n",
    "\n",
    "Notice that, by default, the text samples contain some message metadata such as\n",
    "`\"headers\"`, `\"footers\"` (signatures) and `\"quotes\"` to other posts. We use the\n",
    "`remove` parameter from `fetch_20newsgroups` to strip those features and have a more\n",
    "sensible clustering problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a556c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "categories = [\n",
    "    \"alt.atheism\",\n",
    "    \"talk.religion.misc\",\n",
    "    \"comp.graphics\",\n",
    "    \"sci.space\",\n",
    "]\n",
    "\n",
    "dataset = fetch_20newsgroups(\n",
    "    remove=(\"headers\", \"footers\", \"quotes\"),\n",
    "    subset=\"all\",\n",
    "    categories=categories,\n",
    "    shuffle=True,\n",
    "    random_state=42,\n",
    ")\n",
    "\n",
    "labels = dataset.target\n",
    "unique_labels, category_sizes = np.unique(labels, return_counts=True)\n",
    "true_k = unique_labels.shape[0]\n",
    "\n",
    "print(f\"{len(dataset.data)} documents - {true_k} categories\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b671a738",
   "metadata": {},
   "source": [
    "\n",
    "## Quantifying the quality of clustering results\n",
    "\n",
    "In this section we define a function to score different clustering pipelines\n",
    "using several metrics.\n",
    "\n",
    "Clustering algorithms are fundamentally unsupervised learning methods.\n",
    "However, since we happen to have class labels for this specific dataset, it is\n",
    "possible to use evaluation metrics that leverage this \"supervised\" ground\n",
    "truth information to quantify the quality of the resulting clusters. Examples\n",
    "of such metrics are the following:\n",
    "\n",
    "- homogeneity, which quantifies how much clusters contain only members of a\n",
    "  single class;\n",
    "- completeness, which quantifies how much members of a given class are\n",
    "  assigned to the same clusters;\n",
    "- V-measure, the harmonic mean of completeness and homogeneity;\n",
    "- Rand-Index, which measures how frequently pairs of data points are grouped\n",
    "  consistently according to the result of the clustering algorithm and the\n",
    "  ground truth class assignment;\n",
    "- Adjusted Rand-Index, a chance-adjusted Rand-Index such that random cluster\n",
    "  assignment have an ARI of 0.0 in expectation.\n",
    "\n",
    "If the ground truth labels are not known, evaluation can only be performed\n",
    "using the model results itself. In that case, the Silhouette Coefficient comes\n",
    "in handy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe187d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from sklearn import metrics\n",
    "from time import time\n",
    "\n",
    "evaluations = []\n",
    "evaluations_std = []\n",
    "\n",
    "\n",
    "def fit_and_evaluate(km, X, name=None, n_runs=5):\n",
    "    name = km.__class__.__name__ if name is None else name\n",
    "\n",
    "    train_times = []\n",
    "    scores = defaultdict(list)\n",
    "    for seed in range(n_runs):\n",
    "        km.set_params(random_state=seed)\n",
    "        t0 = time()\n",
    "        km.fit(X)\n",
    "        train_times.append(time() - t0)\n",
    "        scores[\"Homogeneity\"].append(metrics.homogeneity_score(labels, km.labels_))\n",
    "        scores[\"Completeness\"].append(metrics.completeness_score(labels, km.labels_))\n",
    "        scores[\"V-measure\"].append(metrics.v_measure_score(labels, km.labels_))\n",
    "        scores[\"Adjusted Rand-Index\"].append(\n",
    "            metrics.adjusted_rand_score(labels, km.labels_)\n",
    "        )\n",
    "        scores[\"Silhouette Coefficient\"].append(\n",
    "            metrics.silhouette_score(X, km.labels_, sample_size=2000)\n",
    "        )\n",
    "    train_times = np.asarray(train_times)\n",
    "\n",
    "    print(f\"clustering done in {train_times.mean():.2f} ± {train_times.std():.2f} s \")\n",
    "    evaluation = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.mean(),\n",
    "    }\n",
    "    evaluation_std = {\n",
    "        \"estimator\": name,\n",
    "        \"train_time\": train_times.std(),\n",
    "    }\n",
    "    for score_name, score_values in scores.items():\n",
    "        mean_score, std_score = np.mean(score_values), np.std(score_values)\n",
    "        print(f\"{score_name}: {mean_score:.3f} ± {std_score:.3f}\")\n",
    "        evaluation[score_name] = mean_score\n",
    "        evaluation_std[score_name] = std_score\n",
    "    evaluations.append(evaluation)\n",
    "    evaluations_std.append(evaluation_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88e84e29",
   "metadata": {},
   "source": [
    "\n",
    "## K-means clustering on text features\n",
    "\n",
    "We use `TfidfVectorizer` to vectorize the text information. `TfidfVectorizer` uses an\n",
    "in-memory vocabulary (a Python dict) to map the most frequent words to features\n",
    "indices and hence compute a word occurrence frequency (sparse) matrix. The word\n",
    "frequencies are then reweighted using the Inverse Document Frequency (IDF) vector\n",
    "collected feature-wise over the corpus.\n",
    "\n",
    "Furthermore it is possible to post-process those extracted features using\n",
    "dimensionality reduction. We will explore the impact of those choices on the\n",
    "clustering quality in the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8c6f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(\n",
    "    max_df=0.5,\n",
    "    min_df=5,\n",
    "    stop_words=\"english\",\n",
    ")\n",
    "t0 = time()\n",
    "X_tfidf = vectorizer.fit_transform(dataset.data)\n",
    "\n",
    "print(f\"vectorization done in {time() - t0:.3f} s\")\n",
    "print(f\"n_samples: {X_tfidf.shape[0]}, n_features: {X_tfidf.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87402dd5",
   "metadata": {},
   "source": [
    "\n",
    "After ignoring terms that appear in more than 50% of the documents (as set by\n",
    "`max_df=0.5`) and terms that are not present in at least 5 documents (set by\n",
    "`min_df=5`), the resulting number of unique terms `n_features` is around 8,000. We can\n",
    "additionally quantify the sparsity of the `X_tfidf` matrix as the fraction of non-zero\n",
    "entries divided by the total number of elements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f4c981",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"{X_tfidf.nnz / np.prod(X_tfidf.shape):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83b6c256",
   "metadata": {},
   "source": [
    "\n",
    "We find that around 0.7% of the entries of the `X_tfidf` matrix are non-zero.\n",
    "\n",
    "### Clustering sparse data with k-means\n",
    "\n",
    "As both `KMeans` and `MiniBatchKMeans` optimize a non-convex objective function, their\n",
    "clustering is not guaranteed to be optimal for a given random init. Even further, on\n",
    "sparse high-dimensional data such as text vectorized using the Bag of Words approach,\n",
    "k-means can initialize centroids on extremely isolated data points. Those data points\n",
    "can stay their own centroids all along.\n",
    "\n",
    "The following code illustrates how the previous phenomenon can sometimes lead to\n",
    "highly imbalanced clusters, depending on the random initialization:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09f27a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "for seed in range(5):\n",
    "    kmeans = KMeans(\n",
    "        n_clusters=true_k,\n",
    "        max_iter=100,\n",
    "        n_init=1,\n",
    "        random_state=seed,\n",
    "    ).fit(X_tfidf)\n",
    "    cluster_ids, cluster_sizes = np.unique(kmeans.labels_, return_counts=True)\n",
    "    print(f\"Number of elements assigned to each cluster: {cluster_sizes}\")\n",
    "print()\n",
    "print(\n",
    "    \"True number of documents in each category according to the class labels: \"\n",
    "    f\"{category_sizes}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f73f6ba9",
   "metadata": {},
   "source": [
    "\n",
    "To avoid this problem, one possibility is to increase the number of runs with\n",
    "independent random initiations `n_init`. In such case the clustering with the\n",
    "best inertia (objective function of k-means) is chosen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1753737",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    n_clusters=true_k,\n",
    "    max_iter=100,\n",
    "    n_init=5,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, X_tfidf, name=\"KMeans\\non tf-idf vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9c8134",
   "metadata": {},
   "source": [
    "\n",
    "All those clustering evaluation metrics have a maximum value of 1.0 (for a perfect\n",
    "clustering result). Higher values are better. Values of the Adjusted Rand-Index close\n",
    "to 0.0 correspond to a random labeling. Notice from the scores above that the cluster\n",
    "assignment is indeed well above chance level, but the overall quality can certainly\n",
    "improve.\n",
    "\n",
    "Keep in mind that the class labels may not reflect accurately the document topics and\n",
    "therefore metrics that use labels are not necessarily the best to evaluate the quality\n",
    "of our clustering pipeline.\n",
    "\n",
    "### Performing dimensionality reduction using LSA\n",
    "\n",
    "A `n_init=1` can still be used as long as the dimension of the vectorized space is\n",
    "reduced first to make k-means more stable. For such purpose we use `TruncatedSVD`,\n",
    "which works on term count/tf-idf matrices. Since SVD results are not normalized, we\n",
    "redo the normalization to improve the `KMeans` result. Using SVD to reduce the\n",
    "dimensionality of TF-IDF document vectors is often known as [latent semantic\n",
    "analysis](https://en.wikipedia.org/wiki/Latent_semantic_analysis) (LSA) in the\n",
    "information retrieval and text mining literature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f3ccb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "\n",
    "lsa = make_pipeline(TruncatedSVD(n_components=100), Normalizer(copy=False))\n",
    "t0 = time()\n",
    "X_lsa = lsa.fit_transform(X_tfidf)\n",
    "explained_variance = lsa[0].explained_variance_ratio_.sum()\n",
    "\n",
    "print(f\"LSA done in {time() - t0:.3f} s\")\n",
    "print(f\"Explained variance of the SVD step: {explained_variance * 100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1a27c0a",
   "metadata": {},
   "source": [
    "\n",
    "Using a single initialization means the processing time will be reduced for both\n",
    "`KMeans` and `MiniBatchKMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d71adaa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(\n",
    "    n_clusters=true_k,\n",
    "    max_iter=100,\n",
    "    n_init=1,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(kmeans, X_lsa, name=\"KMeans\\nwith LSA on tf-idf vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57116265",
   "metadata": {},
   "source": [
    "\n",
    "We can observe that clustering on the LSA representation of the document is\n",
    "significantly faster (both because of `n_init=1` and because the dimensionality of the\n",
    "LSA feature space is much smaller). Furthermore, all the clustering evaluation metrics\n",
    "have improved. We repeat the experiment with `MiniBatchKMeans`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3655a8be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "\n",
    "minibatch_kmeans = MiniBatchKMeans(\n",
    "    n_clusters=true_k,\n",
    "    n_init=1,\n",
    "    init_size=1000,\n",
    "    batch_size=1000,\n",
    ")\n",
    "\n",
    "fit_and_evaluate(\n",
    "    minibatch_kmeans,\n",
    "    X_lsa,\n",
    "    name=\"MiniBatchKMeans\\nwith LSA on tf-idf vectors\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ed9cb9",
   "metadata": {},
   "source": [
    "\n",
    "### Top terms per cluster\n",
    "\n",
    "Since `TfidfVectorizer` can be inverted we can identify the cluster centers, which\n",
    "provide an intuition of the most influential words **for each cluster**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "849447fc",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "outputs": [],
   "source": [
    "original_space_centroids = lsa[0].inverse_transform(kmeans.cluster_centers_)\n",
    "order_centroids = original_space_centroids.argsort()[:, ::-1]\n",
    "terms = vectorizer.get_feature_names_out()\n",
    "\n",
    "for i in range(true_k):\n",
    "    print(f\"Cluster {i}: \", end=\"\")\n",
    "    for ind in order_centroids[i, :10]:\n",
    "        print(f\"{terms[ind]} \", end=\"\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dba132a",
   "metadata": {},
   "source": [
    "\n",
    "Both methods lead to good results that are similar to running the same models\n",
    "on the traditional LSA vectors (without hashing).\n",
    "\n",
    "### Clustering evaluation summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84bf1f43",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, (ax0, ax1) = plt.subplots(ncols=2, figsize=(16, 6), sharey=True)\n",
    "\n",
    "df = pd.DataFrame(evaluations[::-1]).set_index(\"estimator\")\n",
    "df_std = pd.DataFrame(evaluations_std[::-1]).set_index(\"estimator\")\n",
    "\n",
    "df.drop(\n",
    "    [\"train_time\"],\n",
    "    axis=\"columns\",\n",
    ").plot.barh(ax=ax0, xerr=df_std)\n",
    "ax0.set_xlabel(\"Clustering scores\")\n",
    "ax0.set_ylabel(\"\")\n",
    "\n",
    "df[\"train_time\"].plot.barh(ax=ax1, xerr=df_std[\"train_time\"])\n",
    "ax1.set_xlabel(\"Clustering time (s)\")\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
