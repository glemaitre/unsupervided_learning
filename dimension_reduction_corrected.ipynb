{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9e8126fa",
   "metadata": {},
   "source": [
    "\n",
    "## Dimensionality reduction\n",
    "\n",
    "In this lecture, we briefly present the concept of dimensionality reduction.\n",
    "Dimensionality reduction serves three main purposes:\n",
    "\n",
    "* It can be used to visualize high dimensional data into 2D or 3D in order to\n",
    "  get some insights about the data.\n",
    "* It can be used to remove some noise from the data. In this case, the\n",
    "  dimensionality reduction algorithm will be used as a pre-processing step\n",
    "  before training a supervised learning model.\n",
    "* It can alleviate the curse of dimensionality. In this case, the\n",
    "  dimensionality reduction algorithm will be used as a pre-processing step\n",
    "  before training a supervised learning model.\n",
    "\n",
    "We will present a first technique called Principal Component Analysis (PCA).\n",
    "\n",
    "## Principal Component Analysis\n",
    "\n",
    "The idea behind PCA is to find a new set of axes, ordered by importance, on\n",
    "which we can project the data. The first axis will be the axis along which\n",
    "the data vary the most. The second axis will be the axis orthogonal to the\n",
    "first one that explain the largest amount of remaining variance. And so on\n",
    "until we have as many axes as original features.\n",
    "\n",
    "Let's generate a ellipsoid dataset not centered at (0, 0) to illustrate the\n",
    "concept of PCA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3077c9ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "rng = np.random.default_rng(0)\n",
    "mean = [10, 10]\n",
    "cov = np.array([[6, -3], [-3, 3.5]])\n",
    "X = rng.multivariate_normal(mean=mean, cov=cov, size=1_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad1796cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1])\n",
    "plt.axis(\"square\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff69bdbf",
   "metadata": {},
   "source": [
    "\n",
    "So here now, we will apply PCA on this dataset. We will keep the same number\n",
    "of dimensions as the original dataset and just observe what type of projection\n",
    "we obtain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4a2ee79",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA()\n",
    "X_transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03e50a45",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=2, figsize=(10, 5), sharex=True, sharey=True)\n",
    "\n",
    "axs[0].scatter(X[:, 0], X[:, 1])\n",
    "axs[0].set_xlabel(\"Original feature 1\")\n",
    "axs[0].set_ylabel(\"Original feature 2\")\n",
    "\n",
    "axs[1].scatter(X_transformed[:, 0], X_transformed[:, 1])\n",
    "axs[1].set_xlabel(\"First principal component\")\n",
    "_ = axs[1].set_ylabel(\"Second principal component\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe6f489e",
   "metadata": {},
   "source": [
    "\n",
    "So we see that PCA will apply a rotation and a scaling to the original data\n",
    "such that the first axis will be the axis along which the data vary the most.\n",
    "The second axis will be the axis orthogonal to the first one that explain the\n",
    "largest amount of remaining variance. And so on until we have as many axes as\n",
    "original features.\n",
    "\n",
    "Indeed, we can get this information by inspecting the `explained_variance_`\n",
    "attribute of the PCA object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aeef9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6c703c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77dade9a",
   "metadata": {},
   "source": [
    "\n",
    "So the first component explains more than 85% of the variance in the data.\n",
    "\n",
    "*Load the iris dataset and apply a PCA to project the 4-dimensional data to\n",
    "a 2-dimensional space. Plot the projected data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d107d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "\n",
    "iris = load_iris(as_frame=True)\n",
    "X, y = iris.data, iris.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c782b51a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=2)\n",
    "X_transformed = pca.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9b53904",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X_transformed[:, 0], X_transformed[:, 1], c=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cadd00c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.pairplot(data=iris.frame, hue=\"target\")"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
