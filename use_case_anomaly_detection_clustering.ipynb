{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d3eb2c3",
   "metadata": {},
   "source": [
    "\n",
    "# Customer segmentation use case\n",
    "\n",
    "This example shows a use case where we are interested at segmenting customers.\n",
    "Customer segmentation is principally useful for marketing strategies.\n",
    "\n",
    "We use the UCI dataset available at:\n",
    "https://archive.ics.uci.edu/dataset/352/online+retail\n",
    "\n",
    "This is a transnational data set which contains all the transactions\n",
    "occurring between 01/12/2010 and 09/12/2011 for a UK-based and registered\n",
    "non-store online retail.\n",
    "\n",
    "The following of this use-case is subdivided into 2 parts. At first, we focus\n",
    "on analyzing the dataset and show how to detect outliers using on of the\n",
    "outlier detection algorithm. Subsequently, we will preprocess our dataset to\n",
    "extract marketing relevant features (Recency, Frequency, Monetary) and\n",
    "use a clustering algorithm to segment our customers.\n",
    "\n",
    "## Outlier detection\n",
    "\n",
    "Let's first load the dataset and look at what we got."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d11a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_excel(\"data/online_retail.xlsx\")\n",
    "df = df.convert_dtypes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fa009cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42095dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b758bc3b",
   "metadata": {},
   "source": [
    "\n",
    "From the information above, we see that we have ~550k samples and 8 features. A\n",
    "sample corresponds to a customer transaction. It could be noted that a customer\n",
    "can have multiple transactions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7af022",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"CustomerID\"].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3a8ee4b",
   "metadata": {},
   "source": [
    "\n",
    "From the information above, we see that we have missing values in some of the\n",
    "features, notably in the \"CustomerID\" feature. Since, we have a large number of\n",
    "samples, we drop the samples with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b79d45c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna(axis=0)\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b739324b",
   "metadata": {},
   "source": [
    "\n",
    "Therefore, we have ~400k samples left. Let's look at the distribution of the\n",
    "features. In terms of numeric features, we particularly interested in the \"UnitPrice\"\n",
    "and \"Quantity\" features that will be used later for the customer segmentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6605740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.hist(df[\"Quantity\"], bins=20)\n",
    "plt.xlabel(\"Quantity\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "_ = plt.title(\"Distribution of the Quantity feature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa12026e",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(df[\"UnitPrice\"], bins=20)\n",
    "plt.xlabel(\"Unit price\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "_ = plt.title(\"Distribution of the UnitPrice feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d430b224",
   "metadata": {},
   "source": [
    "\n",
    "We observe that the range of both features is very large while all the samples are\n",
    "concentrated in a few bins. This is typical case that some extreme values are present\n",
    "in the dataset. Let's use an outlier detection algorithm to automatically detect such\n",
    "samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d12ececb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "model = IsolationForest(n_estimators=400)\n",
    "y_pred = model.fit_predict(df[[\"Quantity\", \"UnitPrice\"]])\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144f8675",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427803cb",
   "metadata": {},
   "source": [
    "\n",
    "We used `IsolationForest` to detect the outliers. Looking at the predictions of this\n",
    "estimator, we see that it provides 2 types of output: 1 for inliers and -1 for\n",
    "outliers. It means that for our further processing, we use only the inliers.\n",
    "We will have a closer look at the distribution of the previous features for the\n",
    "inliers and outliers to see what type of data are detected by the algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c463e",
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers = df[y_pred == 1]\n",
    "outliers = df[y_pred == -1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "637b3584",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, axs = plt.subplots(ncols=2, figsize=(12, 4))\n",
    "\n",
    "axs[0].hist(inliers[\"Quantity\"], bins=20)\n",
    "axs[0].set_xlabel(\"Quantity\")\n",
    "axs[0].set_ylabel(\"Frequency\")\n",
    "axs[0].set_title(\"Distribution of the Quantity feature for inliers\")\n",
    "\n",
    "axs[1].hist(inliers[\"UnitPrice\"], bins=20)\n",
    "axs[1].set_xlabel(\"Unit price\")\n",
    "axs[1].set_ylabel(\"Frequency\")\n",
    "_ = axs[1].set_title(\"Distribution of the UnitPrice feature for outliers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9382af67",
   "metadata": {},
   "source": [
    "\n",
    "Now, we observe that we have a more reasonable distribution for the inliers.\n",
    "Without any surprised, the extreme values are detected as outliers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401b934a",
   "metadata": {},
   "outputs": [],
   "source": [
    "outliers[[\"Quantity\", \"UnitPrice\"]].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf7e2504",
   "metadata": {},
   "source": [
    "\n",
    "So the dataset that we will use for the customer segmentation is the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bda0cd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "inliers.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dd0e299",
   "metadata": {
    "lines_to_next_cell": 2
   },
   "source": [
    "\n",
    "## Customer segmentation\n",
    "\n",
    "Now, we use the inliers to extract marketing relevant features and use a clustering\n",
    "algorithm to segment our customers. In this regard, we use the Recency, Frequency,\n",
    "Monetary (RFM) model.\n",
    "\n",
    "The Recency feature corresponds to the number of days since the last transaction of a\n",
    "customer. The lower the recency, the more recent the transaction is and the more\n",
    "likely the customer is active. Let's compute this feature and store it in a dataframe.\n",
    "\n",
    "Since a customer will have several transactions, we will consider the latest\n",
    "transaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcd53976",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_date = inliers[\"InvoiceDate\"].max()\n",
    "recency = (reference_date - inliers[\"InvoiceDate\"]).to_frame()\n",
    "recency[\"CustomerID\"] = inliers[\"CustomerID\"]\n",
    "recency = recency.groupby(\"CustomerID\").min()\n",
    "recency = recency.rename(columns={\"InvoiceDate\": \"Recency\"})\n",
    "recency[\"Recency\"] = recency[\"Recency\"].dt.days\n",
    "recency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179b62f6",
   "metadata": {},
   "source": [
    "\n",
    "The Frequency feature corresponds to the number of transactions of a customer. The\n",
    "higher the frequency, the more likely the customer is active. Let's compute this\n",
    "feature and store it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e346ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "frequency = (\n",
    "    inliers.groupby([\"CustomerID\", \"InvoiceDate\"]).count().groupby(\"CustomerID\").count()\n",
    ")\n",
    "frequency = frequency[[\"InvoiceNo\"]].rename(columns={\"InvoiceNo\": \"Frequency\"})\n",
    "frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4edeff02",
   "metadata": {},
   "source": [
    "\n",
    "The Monetary feature corresponds to the total amount spent by a customer. The higher\n",
    "the monetary, the more likely the customer is active. Let's compute this feature and\n",
    "store it in a dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "034a55a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "monetary = inliers[[\"CustomerID\", \"Quantity\", \"UnitPrice\"]].copy()\n",
    "monetary[\"Monetary\"] = monetary[\"Quantity\"] * monetary[\"UnitPrice\"]\n",
    "monetary = monetary.drop(columns=[\"Quantity\", \"UnitPrice\"])\n",
    "monetary = monetary.groupby(\"CustomerID\").sum()\n",
    "monetary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ac706ff",
   "metadata": {},
   "source": [
    "\n",
    "Now, we merge all 3 features together in a single dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72febce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm = pd.concat([recency, frequency, monetary], axis=1)\n",
    "rfm.index = rfm.index.astype(int)\n",
    "rfm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b850f0c4",
   "metadata": {},
   "source": [
    "\n",
    "At this stage, we are ready to use a clustering algorithm to segment our customers.\n",
    "Regrouping the customers in cluster allow us to apply different strategy depending on\n",
    "the cluster characteristics. For instance, we can target the customers that are\n",
    "inactive for a long time with a special offer to try to reactivate them.\n",
    "\n",
    "We use the `KMeans` algorithm to perform the clustering. However, there is two\n",
    "important things to consider before analyzing the results of the clustering.\n",
    "\n",
    "First, we should look at the scale of our features:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c74d143",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfm.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdace18",
   "metadata": {},
   "source": [
    "\n",
    "We observe that we have different range for the three features. `KMeans` will use\n",
    "an Euclidean distance to compute the distance between two samples. Therefore, the\n",
    "distance will be dominated by the \"Monetary\" feature because is dynamic range is\n",
    "much larger. If we want each feature to contribute equally to the clustering, we\n",
    "should therefore scale the features to the same range before applying the clustering.\n",
    "\n",
    "Then, we should also select the number of clusters to use. We will try to apply the\n",
    "elbow method and check where to we go from there. It means that we will fit several\n",
    "`KMeans` model and look at the inertia to select an appropriate number of clusters.\n",
    "\n",
    "Let's start by creating a pipeline that scale the data and then cluster it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7885a49d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "cluster = Pipeline(steps=[\n",
    "    (\"scaler\", StandardScaler()), (\"kmeans\", KMeans(n_init=10))\n",
    "])\n",
    "cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89324bad",
   "metadata": {},
   "source": [
    "\n",
    "Now, we cluster the dataset with a different number of clusters and store the inertia."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "025aed3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "inertia = {}\n",
    "for n_clusters in range(1, 20):\n",
    "    cluster.set_params(kmeans__n_clusters=n_clusters)\n",
    "    cluster.fit(rfm)\n",
    "    inertia[n_clusters] = cluster[-1].inertia_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "931cc011",
   "metadata": {},
   "source": [
    "\n",
    "Finally, let's check the inertia for each number of clusters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96e0797",
   "metadata": {},
   "outputs": [],
   "source": [
    "_, ax = plt.subplots()\n",
    "ax.plot(list(inertia.keys()), list(inertia.values()), marker=\"o\")\n",
    "ax.set_xlabel(\"Number of clusters\")\n",
    "ax.set_ylabel(\"Inertia\")\n",
    "_ = ax.set_xticks(range(1, 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "491d602d",
   "metadata": {},
   "source": [
    "\n",
    "It is not super obvious which number of cluster to select from this plot. A number\n",
    "between 4 and 7 seems reasonable. We can first have a try with 6 clusters and check\n",
    "the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d658ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.set_params(kmeans__n_clusters=6).fit(rfm)\n",
    "prototype_customer = pd.DataFrame(\n",
    "    cluster[0].inverse_transform(cluster[-1].cluster_centers_),\n",
    "    columns=rfm.columns,\n",
    ")\n",
    "prototype_customer[\"Number of customer\"] = pd.Series(Counter(cluster[-1].labels_))\n",
    "prototype_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c83d664",
   "metadata": {},
   "source": [
    "\n",
    "We observe that with 6 clusters, we have 2 clusters with very few customers.\n",
    "Therefore, we reduce the number of clusters to 4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608c4720",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster.set_params(kmeans__n_clusters=4).fit(rfm)\n",
    "prototype_customer = pd.DataFrame(\n",
    "    cluster[0].inverse_transform(cluster[-1].cluster_centers_),\n",
    "    columns=rfm.columns,\n",
    ")\n",
    "prototype_customer[\"Number of customer\"] = pd.Series(Counter(cluster[-1].labels_))\n",
    "prototype_customer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45831fc",
   "metadata": {},
   "source": [
    "\n",
    "The cluster #2 corresponds to our most active customers. The recency is really low and\n",
    "the amount spent is really high. The cluster #3 are probably the most important\n",
    "customers: they are active, spend a lot money and account for ~7% of the sells.\n",
    "If we consider the Pareto principle, we should focus on the cluster #1, #2 and #3\n",
    "since they would account for a large part of the sells and account for 30% of the\n",
    "customers."
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "cell_metadata_filter": "-all",
   "main_language": "python",
   "notebook_metadata_filter": "-all"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
